Today, I evaluated the chatbot’s responses to a set of pre-defined questions. This process is critical to understand how well the model performs in different contexts and identify areas for improvement. I created a list of common user queries and fed them to the chatbot, noting how accurate and coherent the responses were. While the chatbot did well in understanding straightforward questions, it struggled with more complex ones, particularly those requiring in-depth knowledge or multi-turn conversations. I also analyzed the balance between retrieval and generation in the chatbot’s responses, particularly how well it utilized the RAG system to fetch relevant information. This evaluation session provided me with a clear understanding of the model’s strengths and weaknesses. Moving forward, I plan to make the necessary adjustments in the model’s training and data handling to improve its performance in real-world scenarios.
